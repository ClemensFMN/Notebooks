{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db35565c-0879-4b4b-abc6-8f63f2a8b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "torchvision version: 0.18.1a0\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a22537-ffa5-486d-83af-09f2fcdd0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dae83d2-12cb-4bc1-8256-4d827865424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7200fe-b9c5-4b06-88ba-8f0aea590e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[1]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e62041b-de96-4dd4-aee4-1db3ad5d6d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x73b746142ea0>, <torch.utils.data.dataloader.DataLoader object at 0x73b746b9e510>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db6742a-18a8-4dfe-b728-e2b78a41a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde28c51-d065-467a-a823-ceecc5ca97b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hidden layer\n",
    "    output_shape=len(class_names) # one for every class\n",
    ")\n",
    "model_0.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d4c749-bae4-4fcb-a748-7fcb38df328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ccd2aa-54da-46af-a907-2d8fbc8227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4971d1-ac74-4b97-a7c5-b7b231725d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57e45ba4-b5d6-4687-84f1-883f00da7a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8670719876f34347b22e2e51ef3e6046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.44180 | Train accuracy: 84.69%\n",
      "Test loss: 0.48127 | Test accuracy: 83.18%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.43549 | Train accuracy: 84.82%\n",
      "Test loss: 0.47527 | Test accuracy: 83.49%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.42998 | Train accuracy: 84.86%\n",
      "Test loss: 0.48396 | Test accuracy: 83.53%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_0, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_0,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc18a06f-4b6f-4081-acb3-9a79d77f6582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label =  Ankle boot predicted label =  Ankle boot\n",
      "true label =  T-shirt/top predicted label =  T-shirt/top\n",
      "true label =  T-shirt/top predicted label =  T-shirt/top\n",
      "true label =  Dress predicted label =  T-shirt/top\n",
      "true label =  T-shirt/top predicted label =  Dress\n",
      "true label =  Pullover predicted label =  Pullover\n",
      "true label =  Sneaker predicted label =  Sneaker\n",
      "true label =  Pullover predicted label =  Pullover\n",
      "true label =  Sandal predicted label =  Sandal\n",
      "true label =  Sandal predicted label =  Sandal\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    image, label = train_data[i]\n",
    "    res = model_0(image)\n",
    "    \n",
    "    \n",
    "    print(\"true label = \", class_names[label], \"predicted label = \", class_names[res.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "701cc582-2a74-461e-9414-80313da0070e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV2(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a convolutional neural network \n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape=1, \n",
    "    hidden_units=10, \n",
    "    output_shape=len(class_names))\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f29c9d0c-b00f-40e9-b848-a4bac7a641c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), \n",
    "                             lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab1157c0-8f1c-41a0-8f5e-614f7a44bb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23ecc9881df4920ad94e060f85937dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.60429 | Train accuracy: 78.01%\n",
      "Test loss: 0.39194 | Test accuracy: 85.50%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.36807 | Train accuracy: 86.65%\n",
      "Test loss: 0.36334 | Test accuracy: 87.16%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.32998 | Train accuracy: 87.97%\n",
      "Test loss: 0.34097 | Test accuracy: 87.47%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_2, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
