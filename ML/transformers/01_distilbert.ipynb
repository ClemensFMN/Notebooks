{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a0dd0b-8f21-4e30-a150-a0cc231f8f01",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Play around with the distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027e2373-4890-4e75-b369-67a5be582c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceddf231-1276-4e6e-b934-b0fed4386a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05879a0e-952a-44d4-ade6-783ee5801aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68addce-112a-4fa3-b766-9882761820ea",
   "metadata": {},
   "source": [
    "That's the pipeline way of doing things:\n",
    "\n",
    "- Create a pipeline for text classification\n",
    "- Stuff some text into the pipeline\n",
    "- Print output (whatever the meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a7cb07d-5067-438f-be8b-1344fb85397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.4764806926250458}, {'label': 'LABEL_1', 'score': 0.5235193371772766}]]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model_ckpt,\n",
    "    dtype=torch.float16,\n",
    "    device=0,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "result = classifier(\"I love using Hugging Face Transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72100a03-ffbd-4920-ace2-cc72c8bf4052",
   "metadata": {},
   "source": [
    "Now let's do it the pytorch way...\n",
    "\n",
    "- Load the model\n",
    "- input some token to the model\n",
    "- calculate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a52a105-3612-466c-956a-6854ffd20b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = (AutoModel\n",
    "         .from_pretrained(model_ckpt)\n",
    "         .to(device))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7651d419-39cb-4b7d-b104-925fbb4b8890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2070, 6057, 3793, 2005, 5604,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tks = tokenizer(text, return_tensors=\"pt\")\n",
    "tks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00ec72da-359e-473a-9ee8-e70b0fdbfcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tks)\n",
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c932b-1561-4346-9ae2-ca92ba6a2957",
   "metadata": {},
   "source": [
    "This seems to be one hidden state vector (of size 768) for each token (there are 7 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c595cf-5398-4d33-a341-398ab62a26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba159b-404c-4fdf-af77-84e216f3bcb3",
   "metadata": {},
   "source": [
    "Save as above & open / visualize in https://netron.app/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
